{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ9nsm/VKRZXLopy+Jog9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chanchal56/Codesoft-projects/blob/main/movie_recommended_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NhPs3ikCw8Pp"
      },
      "outputs": [],
      "source": [
        "! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o ml-latest-small.zip\n",
        "import zipfile\n",
        "with zipfile.ZipFile('ml-latest-small.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "movies_df = pd.read_csv('data/ml-latest-small/movies.csv')\n",
        "ratings_df = pd.read_csv('data/ml-latest-small/ratings.csv')\n",
        "print('The dimensions of movies dataframe are:', movies_df.shape,'\\nThe dimensions of ratings dataframe are:', ratings_df.shape)\n",
        "\n",
        "\n",
        "movies_df.head()\n",
        "\n",
        "ratings_df.head()\n",
        "\n",
        "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
        "n_users = len(ratings_df.userId.unique())\n",
        "n_items = len(ratings_df.movieId.unique())\n",
        "print(\"Number of unique users:\", n_users)\n",
        "print(\"Number of unique movies:\", n_items)\n",
        "print(\"The full rating matrix will have:\", n_users*n_items, 'elements.')\n",
        "print('----------')\n",
        "print(\"Number of ratings:\", len(ratings_df))\n",
        "print(\"Therefore: \", len(ratings_df) / (n_users*n_items) * 100, '% of the matrix is filled.')\n",
        "print(\"We have an incredibly sparse matrix to work with here.\")\n",
        "print(\"And... as you can imagine, as the number of users and products grow, the number of elements will increase by n*2\")\n",
        "print(\"You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\")\n",
        "print(\"One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "class MatrixFactorization(torch.nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=20):\n",
        "        super().__init__()\n",
        "\n",
        "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
        "\n",
        "        self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
        "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        users, items = data[:,0], data[:,1]\n",
        "        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
        "\n",
        "    def predict(self, user, item):\n",
        "        return self.forward(user, item)\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class Loader(Dataset):\n",
        "    def __init__(self):\n",
        "              self.ratings = ratings_df.copy()\n",
        "\n",
        "              users = ratings_df.userId.unique()\n",
        "              movies = ratings_df.movieId.unique()\n",
        "\n",
        "\n",
        "              self.userid2idx = {o:i for i,o in enumerate(users)}\n",
        "              self.movieid2idx = {o:i for i,o in enumerate(movies)}\n",
        "\n",
        "              self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
        "              self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}\n",
        "\n",
        "\n",
        "              self.ratings.movieId = ratings_df.movieId.apply(lambda x: self.movieid2idx[x])\n",
        "              self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
        "\n",
        "\n",
        "              self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
        "              self.y = self.ratings['rating'].values\n",
        "              self.x, self.y = torch.tensor(self.x), torch.tensor(self.y)\n",
        "def __getitem__(self, index):\n",
        "  return (self.x[index], self.y[index])\n",
        "\n",
        "def __len__(self):\n",
        "              return len(self.ratings)\n",
        "num_epochs = 128\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "print(\"Is running on GPU:\", cuda)\n",
        "\n",
        "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
        "print(model)\n",
        "for name, param in model.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    print(name, param.data)\n",
        "\n",
        "if cuda:\n",
        "                model = model.cuda()\n",
        "\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "train_set = Loader()\n",
        "train_loader = DataLoader(train_set, 128, shuffle=True)\n",
        "for it in tqdm(range(num_epochs)):\n",
        "  losses = []\n",
        "  for x, y in train_loader:\n",
        "       if cuda:\n",
        "          x, y = x.cuda(), y.cuda()\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(x)\n",
        "          loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
        "          losses.append(loss.item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "  print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))\n",
        "\n",
        "  c = 0\n",
        "  uw = 0\n",
        "  iw = 0\n",
        "  for name, param in model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "          print(name, param.data)\n",
        "          if c == 0:\n",
        "            uw = param.data\n",
        "            c +=1\n",
        "          else:\n",
        "            iw = param.data\n",
        "\n",
        "trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()\n",
        "\n",
        "\n",
        "len(trained_movie_embeddings)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)\n",
        "\n",
        "\n",
        "'''It can be seen here that the movies that are in the same cluster tend to have\n",
        "    similar genres. Also note that the algorithm is unfamiliar with the movie name\n",
        "    and only obtained the relationships by looking at the numbers representing how\n",
        "    users have responded to the movie selections.'''\n",
        "for cluster in range(10):\n",
        "      print(\"Cluster #{}\".format(cluster))\n",
        "      movs = []\n",
        "      for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
        "        movid = train_set.idx2movieid[movidx]\n",
        "        rat_count = ratings_df.loc[ratings_df['movieId']==movid].count()[0]\n",
        "        movs.append((movie_names[movid], rat_count))\n",
        "      for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]:\n",
        "        print(\"\\t\", mov[0])\n"
      ]
    }
  ]
}